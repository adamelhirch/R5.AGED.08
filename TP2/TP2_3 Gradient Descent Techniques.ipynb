{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f301e344-13c0-42a5-a760-d491036d646a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TP 2. Regression linéaire from scratch :\n",
    "## TP 2.3. Descente du gradient (Regression linéaire)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dafceba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Objectif \n",
    "Automatiser le processus d'apprentissage des w et b par descente du gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a50c88",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194867af",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1- Dataset \n",
    "On repart avec les mêmes données \n",
    "\n",
    "| taille (100 )     | Prix (1000s de dollars) |\n",
    "| ----------------| ------------------------ |\n",
    "| 1               | 300                      |\n",
    "| 2               | 500                      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044c527",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load our data set\n",
    "x_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300.0, 500.0])   #target value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b185c4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2- Fonction coût\n",
    "On reprend la version déjà vue en TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee839a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Function to calculate the cost\n",
    "def compute_cost(x, y, w, b):\n",
    "   \n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m) * cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b5c20",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## 3- Désente du gradient \n",
    "### 3.1 Rappel de la descente de gradient\n",
    "Jusqu'à présent dans ce cours, vous avez développé un modèle linéaire qui prédit $f_{w,b}(x^{(i)})$ :\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "Dans la régression linéaire, vous utilisez les données d'apprentissage en entrée pour ajuster les paramètres $w$,$b$ en minimisant une mesure de l'erreur entre nos prédictions $f_{w,b}(x^{(i)})$ et les données réelles $y^{(i)}$. Cette mesure s'appelle le $coût$, $J(w,b)$. \n",
    "Lors de l'apprentissage, vous mesurez le coût sur tous les échantillons d'apprentissage $x^{(i)},y^{(i)}$\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf11f2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "la *descente de gradient* a été décrite comme suit :\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "où les paramètres $w$, $b$ sont mis à jour simultanément.  \n",
    "Le gradient est défini comme suit :\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Ici, *simultanément* signifie que vous calculez les dérivées partielles pour tous les paramètres avant de mettre à jour l'un d'entre eux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4361141",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2 Mise en œuvre de la descente de gradient\n",
    "Vous allez mettre en œuvre l'algorithme de descente de gradient pour une seule caractéristique. Vous aurez besoin de trois fonctions. \n",
    "- `compute_cost` implémentant l'équation (2) ci-dessus (code du laboratoire précédent)\n",
    "- `compute_gradient` implémentant les équations (4) et (5) ci-dessus\n",
    "- `gradient_descent`, utilisant compute_gradient et compute_cost\n",
    "\n",
    "**Conventions**:\n",
    "- Le nom des variables python contenant des dérivées partielles suit ce modèle, $\\frac{\\partial J(w,b)}{\\partial b}$ sera `dj_db`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e7aad-2d3f-458f-a33e-77c8f2a6f71a",
   "metadata": {},
   "source": [
    "#### Question 1 : \n",
    "Ecrire ci dessous le code de compute_gradient (les équations (4) et (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f0fb8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Ecrire ici le code de compute_gradient \n",
    "\n",
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Calcule le gradient pour la régression linéaire \n",
    "    Args :\n",
    "      x (ndarray (m,)) : données, m exemples \n",
    "      y (ndarray (m,)) : valeurs cibles\n",
    "      w,b (scalaire) : paramètres du modèle  \n",
    "    Résultats\n",
    "      dj_dw (scalaire) : Le gradient du coût par rapport au(x) paramètre(s) w\n",
    "      dj_db (scalaire) : Le gradient du coût par rapport au paramètre b     \n",
    "     \"\"\"\n",
    "    # Number of training examples\n",
    "    m = x.shape[0]  \n",
    "    # initialiser dj_dw et dj_db\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = \n",
    "        dj_dw_i = \n",
    "        dj_db_i = \n",
    "        dj_db += \n",
    "        dj_dw += \n",
    "    dj_dw = \n",
    "    dj_db = \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4ff99a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Question 2 : \n",
    "Maintenant que les gradients sont calculés, la descente de gradient, décrite dans l'équation (3) ci-dessus, peut être implémentée ci-dessous dans `gradient_descent`. \n",
    "\n",
    "Ecrire le code de la fonction gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08ff69",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \n",
    "    \"\"\"\n",
    "    Effectue une descente de gradient pour ajuster w,b. Met à jour w,b en prenant \n",
    "    num_iters pas de gradient avec un taux d'apprentissage alpha\n",
    "    \n",
    "    Args :\n",
    "      x (ndarray (m,)) : Données, m exemples \n",
    "      y (ndarray (m,)) : valeurs cibles\n",
    "      w_in,b_in (scalaire) : valeurs initiales des paramètres du modèle  \n",
    "      alpha (float) :     Taux d'apprentissage\n",
    "      num_iters (int) : nombre d'itérations pour exécuter la descente de gradient\n",
    "      cost_function : fonction à appeler pour produire le coût\n",
    "      gradient_function : fonction à appeler pour produire le gradient\n",
    "      \n",
    "    Retourne :\n",
    "      w (scalaire) : Valeur mise à jour du paramètre après l'exécution de la descente de gradient\n",
    "      b (scalaire) : Valeur mise à jour du paramètre après l'exécution de la descente de gradient\n",
    "      J_history (Liste) : Historique des valeurs de coût\n",
    "      p_history (liste) : Historique des paramètres [w,b] \n",
    "      \"\"\"\n",
    "\n",
    "    w = copy.deepcopy(w_in) # évite la modification de w_in\n",
    "    \n",
    "    # Un tableau pour stocker les coûts J et w à chaque itération, \n",
    "    # principalement pour la représentation graphique ultérieure.\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculer le gradient et mettre à jour les paramètres en utilisant gradient_function=copute_gradient\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Mise à jour des paramètres en utilisant l' equation (3) \n",
    "        b =                        \n",
    "        w =                         \n",
    "\n",
    "        # Save le coût J à chaque itération\n",
    "        if i<100000:      \n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "            \n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab095888-6654-47b6-84b8-1b1d7f6da772",
   "metadata": {},
   "source": [
    "#### Le code principal qui fait appel à la fonction gradient_descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a97c70",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# initialisation du nombre d'itérations et la vitesse d'apprentissage.\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)\n",
    "print(\"***********************************\")\n",
    "print(f\"(w,b) les valeurs de w et b  ({w_final:8.4f},{b_final:8.4f})\")\n",
    "print(\"***********************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3323c373",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='darkgreen'><b>Constat ---> Cliquez ici</b></font>\n",
    "</summary>\n",
    "    <p>\n",
    "    <ul>\n",
    "Notez certaines caractéristiques du processus de descente de gradient listé ci-dessus.  \n",
    "\n",
    "- Le coût commence par être élevé et diminue rapidement.\n",
    "- Les dérivées partielles, `dj_dw`, et `dj_db` deviennent également plus petites, rapidement au début, puis plus lentement. .\n",
    "- la progression ralentit bien que le taux d'apprentissage, alpha, reste fixe\n",
    "</ul>\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5916c650",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.3 Coût en fonction des itérations de la descente de gradient \n",
    "Un graphique du coût en fonction du nombre d'itérations est une mesure utile pour visualiser la progression de la descente de gradient. Le coût doit toujours diminuer lorsque les itérations sont réussies. Le changement de coût est si rapide au départ qu'il est utile de représenter la descente initiale sur une échelle différente de celle de la descente finale. Dans les graphiques ci-dessous, notez l'échelle du coût sur les axes et le pas d'itération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe580a0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot cost versus iteration  \n",
    "#fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "plt.plot(J_hist)\n",
    "\n",
    "plt.title(\"Cost vs. iteration(début)\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Cost')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('iteration')\n",
    "#plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3666f97-dad1-4889-a5fb-c353f9bf7203",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='darkgreen'><b>Constat ---> Cliquez ici</b></font>\n",
    "</summary>\n",
    "    <p>\n",
    "    <ul>\n",
    "Difficile d'analyser la courbe. En fait ça déscend très vide les débuts, puis elle ralentit. \n",
    "Pour mieux visuaiser le cout, on trace deux graphiques, le premier représente, le cout au début, pr exemple les 100 premières valeurs de J_hist le deuxième regarde les dernières valeurs à partir de 1000.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29529c0f-1358-441f-ae1a-6a659218305b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\n",
    "ax1.set_title(\"Cost vs. iteration(début)\");  ax2.set_title(\"Cost vs. iteration (fin)\")\n",
    "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa35abca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Prédictions\n",
    "\n",
    "On a trouvé les valeurs optimales des paramètres $w$ et $b$, vous pouvez utiliser le modèle pour prédire les valeurs des logements sur la base des paramètres appris. Comme prévu, les valeurs prédites sont presque identiques aux valeurs d'apprentissage pour le même logement. En outre, la valeur qui ne figure pas dans la prédiction correspond à la valeur attendue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f4199",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"100 m2 house prédiction {w_final*1.0 + b_final:0.1f} Thousand euros\")\n",
    "print(f\"120 m2 house prediction {w_final*1.2 + b_final:0.1f} Thousand euros\")\n",
    "print(f\"200 m2 house prediction {w_final*2.0 + b_final:0.1f} Thousand euros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a073f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5. Impact du taux d'apprentissage/Learning Rate\n",
    "\n",
    "Dans le cours, il y a eu une discussion sur la valeur appropriée du taux d'apprentissage, $\\alpha$ dans l'équation(3). Plus $\\alpha$ est grand, plus la descente de gradient convergera rapidement vers une solution. Mais s'il est trop grand, la descente de gradient divergera. Ci-dessus, vous avez un exemple de solution qui converge bien.\n",
    "\n",
    "### Question : \n",
    "Augmenter la valeur de $\\alpha$ (par exemple $tmp\\_alpha=0.8$) et regarder ce qui se passe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32776e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "w_init = \n",
    "b_init = \n",
    "# set alpha to a large value\n",
    "iterations = 1000\n",
    "\n",
    "tmp_alpha = \n",
    "# run gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dc2a34",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Que remarquez vous ?\n",
    "Tracer la courbe coût versus itération,  \n",
    "Quelles sont les bonnes valeurs des paramètres $w$ et $b$ ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f4f74-b491-4872-8767-653bf3f8fbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot cost versus iteration  \n",
    "#fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "plt.plot(J_hist)\n",
    "\n",
    "plt.title(\"Cost vs. iteration(début)\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Cost')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('iteration')\n",
    "#plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b5fa8-3000-4eb6-b64a-ae2a808dbe30",
   "metadata": {},
   "source": [
    "### Exercice\n",
    "Réecrire les code des fonctions, `compute_cost`, `compute_gradient`, `gradient_descent`en utilisant une vectorisation. \n",
    "Vérifier votre code (appel de la cellule comportant le code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b2619-0550-4835-b302-c092cf7d5190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "40291"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
